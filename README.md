# Hand Gesture Recognition with MediaPipe & Bidirectional LSTM

![Confusion Matrix](confusion_matrix.png)

> **Project type:** Software Development Project (SDP)  
> **Goal:** Train a deepâ€‘learning model that recognises 17 static hand gestures in realâ€‘time from a laptop/USB camera.

---

## âœ¨ Key Features
- **MediaPipe Hands** to extract 21â€‘point hand landmarks from frames.
- **Sequence Assembly** of landmark vectors into fixedâ€‘length `.npy` files (see `dataset_landmark_sequences/`).
- **Bidirectional LSTMâ€‘based classifier** (`lstm_train_model.py`) with cosineâ€‘decay restart scheduler.
- **Earlyâ€‘Stopping & Checkpoints** â€“ best weights saved to `best_lstm_model.keras`.
- **Realâ€‘time Inference** (`real_time_video_capture.py`) with onâ€‘screen annotation **& optional TTS** feedback.
- **Training artefacts** â€“ accuracy/loss curves, ROC curve, confusionâ€‘matrix.

---

## ğŸ“‚ Directory Layout

```text
sdp-project/
â”œâ”€â”€ dataset_landmark_sequences/    # Preâ€‘processed gesture sequences (.npy)
â”œâ”€â”€ lstm_train_model.py            # Modelâ€‘building & training script
â”œâ”€â”€ split_dataset.py               # Train/val/test split helper
â”œâ”€â”€ real_time_video_capture.py     # Webcam demo (inference)
â”œâ”€â”€ best_lstm_model.keras          # â­ best validation checkpoint
â”œâ”€â”€ lstm_trained_model.keras       # Model after final epoch
â”œâ”€â”€ lstm_training_history.json     # Saved history for plots
â”œâ”€â”€ roc_curve.png                  # ROC curve
â”œâ”€â”€ Accuracy and Loss Curves.png   # Training curves
â”œâ”€â”€ confusion_matrix.png           # Confusion matrix
â””â”€â”€ README.md                      # **You are here**
```

---

## ğŸ”§ Requirements

| Package | TestedÂ Version |
|---------|----------------|
| Python  | 3.9 / 3.10     |
| TensorFlow | â‰¥Â 2.12 |
| NumPy | â‰¥Â 1.23 |
| OpenCVâ€‘Python | â‰¥Â 4.10 |
| MediaPipe | â‰¥Â 0.10 |
| scikitâ€‘learn | â‰¥Â 1.3 |
| matplotlib | â‰¥Â 3.8 |
| pyttsx3 (optionalÂ TTS) | â‰¥Â 2.90 |

Create a **`requirements.txt`** from the above or install manually:

```bash
pip install -r requirements.txt
```

---

## ğŸš€ Quickâ€‘Start

```bash
# 1) Clone the repo (after you push it to GitHub)
git clone https://github.com/<YOUR_USERNAME>/sdp-project.git
cd sdp-project

# 2) (Optional) Create & activate a virtual environment
python -m venv venv && source venv/bin/activate  # Windows: venv\Scripts\activate

# 3) Install deps
pip install -r requirements.txt

# 4) Train the model (â‰ˆÂ 5â€“10Â min on GPU â€‘ adjust batch/epochs in script)
python lstm_train_model.py

# 5) Run the realâ€‘time demo
python real_time_video_capture.py
```

> **Note:** The `dataset_landmark_sequences/` folder already contains sample sequences for 17 gestures. Replace or extend with your own data and update the label mapping inside `real_time_video_capture.py` if you add/remove classes.

---

## ğŸ“Š Results

| Metric | Value |
|--------|-------|
| Training accuracy | **99.3â€¯%** |
| Validation accuracy | **92.3â€¯%** |
| Validation loss | 0.23 |
| ROCâ€‘AUC | See `roc_curve.png` |

![Accuracy & Loss](Accuracy%20and%20Loss%20Curves.png)

---

## ğŸ—ï¸ How It Works

1. **Landmark Extraction** â€“ `dataset_landmarks.py` iterates over raw image folders, runs MediaPipe Hands, and stores 21Ã—3 normalised landmark coordinates.
2. **Sequence Building** â€“ consecutive frames are stacked into sequences of equal length (defaultÂ =â€¯30) and saved as `.npy` arrays.
3. **Model** â€“ two Biâ€‘LSTM layers with layerâ€‘norm, dropout and dense softmax output.
4. **Training** â€“ cosineâ€‘decay restarts, earlyâ€‘stopping; class imbalance handled via `class_weight`.
5. **Inference** â€“ webcam frames â†’ landmarks â†’ slidingâ€‘window buffer â†’ model prediction â†’ overlay + optional speech.

---

## ğŸ–¥ï¸ Realâ€‘Time Demo

Run:

```bash
python real_time_video_capture.py
```

- Press **`q`** to quit.
- Predictions with confidence â‰¥â€¯0.7 are displayed.
- Enable audio feedback by keeping `pyttsx3` installed.

![Demo GIF](https://user-images.githubusercontent.com/0000000/demo.gif)

---

## ğŸ“ Acknowledgements
- [MediaPipe](https://github.com/google/mediapipe)
- TensorFlow/Keras
- This work was completed as part of the 6thâ€‘semester **Software Development Project** course at **XYZ University**.

## ğŸ“œ License
Released under the MIT License â€“ see [`LICENSE`](LICENSE) for details.
